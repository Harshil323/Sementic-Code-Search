{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtYQcJiynZgw53wj7Rmpjc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshil323/Sementic-Code-Search/blob/main/SCS_Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmUA_DTPVc5T",
        "outputId": "298568dc-8055-4a54-d5c3-a283d6a932af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# initially we are connect to the google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grab the data and pre-process it"
      ],
      "metadata": {
        "id": "Z0EvDlIAVw4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing all the necessary files\n",
        "import ast\n",
        "import sqlite3\n",
        "import glob\n",
        "import re\n",
        "from pathlib import Path\n",
        "import astor\n",
        "import pandas as pd\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "BM0dVaFQVfJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import ast\n",
        "import traceback\n",
        "# directory name\n",
        "dirname = '/content/gdrive/MyDrive/Dataset/'\n",
        "\n",
        "# create a dataframe for dataset\n",
        "df = pd.DataFrame()\n",
        "df[\"Source_Code\"] = \"Source_code\"\n",
        "df[\"repo_path\"] = \"repo_path\"\n",
        "# giving directory name to Path() function\n",
        "paths = Path(dirname).glob('**/*.py',)\n",
        "\n",
        "# iterating over all files\n",
        "for path in paths:\n",
        "    txt = path.read_text()\n",
        "    # print(type(txt))\n",
        "    # print(path)\n",
        "    # Check whether the systax of the python file is true or not\n",
        "    valid = True\n",
        "    try:\n",
        "        ast.parse(txt)\n",
        "    except SyntaxError:\n",
        "        valid = False\n",
        "        traceback.print_exc()\n",
        "        print(path)\n",
        "    # print(valid)\n",
        "    if valid == True:\n",
        "        df = df.append({'Source_code' : txt,'repo_path':path},ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFWFC52uV4c7",
        "outputId": "475f2b11-cde5-4903-a4f4-c7e1996ac1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-3-e815675ecf1e>\", line 22, in <module>\n",
            "    ast.parse(txt)\n",
            "  File \"/usr/lib/python3.7/ast.py\", line 35, in parse\n",
            "    return compile(source, filename, mode, PyCF_ONLY_AST)\n",
            "  File \"<fstring>\", line 1\n",
            "    (data = )\n",
            "          ^\n",
            "SyntaxError: invalid syntax\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Dataset/3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### in the above example a file with compilation error is found for python so it will be excluded"
      ],
      "metadata": {
        "id": "3Q6dJiGiV_xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "df.drop(['Source_Code'],axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EWYdirD3V-vw",
        "outputId": "82f8a65a-3898-441c-d0d8-75d340a64a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                repo_path  \\\n",
              "0    /content/gdrive/MyDrive/Dataset/1.py   \n",
              "1    /content/gdrive/MyDrive/Dataset/2.py   \n",
              "2    /content/gdrive/MyDrive/Dataset/4.py   \n",
              "3    /content/gdrive/MyDrive/Dataset/5.py   \n",
              "4    /content/gdrive/MyDrive/Dataset/6.py   \n",
              "5    /content/gdrive/MyDrive/Dataset/7.py   \n",
              "6    /content/gdrive/MyDrive/Dataset/8.py   \n",
              "7    /content/gdrive/MyDrive/Dataset/9.py   \n",
              "8   /content/gdrive/MyDrive/Dataset/10.py   \n",
              "9   /content/gdrive/MyDrive/Dataset/11.py   \n",
              "10  /content/gdrive/MyDrive/Dataset/12.py   \n",
              "11  /content/gdrive/MyDrive/Dataset/13.py   \n",
              "12  /content/gdrive/MyDrive/Dataset/14.py   \n",
              "13  /content/gdrive/MyDrive/Dataset/15.py   \n",
              "14  /content/gdrive/MyDrive/Dataset/16.py   \n",
              "15  /content/gdrive/MyDrive/Dataset/17.py   \n",
              "16  /content/gdrive/MyDrive/Dataset/18.py   \n",
              "17  /content/gdrive/MyDrive/Dataset/19.py   \n",
              "18  /content/gdrive/MyDrive/Dataset/20.py   \n",
              "19  /content/gdrive/MyDrive/Dataset/21.py   \n",
              "20  /content/gdrive/MyDrive/Dataset/22.py   \n",
              "21  /content/gdrive/MyDrive/Dataset/23.py   \n",
              "22  /content/gdrive/MyDrive/Dataset/24.py   \n",
              "23  /content/gdrive/MyDrive/Dataset/25.py   \n",
              "24  /content/gdrive/MyDrive/Dataset/26.py   \n",
              "25  /content/gdrive/MyDrive/Dataset/27.py   \n",
              "26  /content/gdrive/MyDrive/Dataset/28.py   \n",
              "27  /content/gdrive/MyDrive/Dataset/29.py   \n",
              "28  /content/gdrive/MyDrive/Dataset/30.py   \n",
              "29  /content/gdrive/MyDrive/Dataset/31.py   \n",
              "30  /content/gdrive/MyDrive/Dataset/32.py   \n",
              "\n",
              "                                          Source_code  \n",
              "0   # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Wed F...  \n",
              "1   # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "2   # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "3   # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "4   # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "5   # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "6   # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "7   # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "8   # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "9   # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "10  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "11  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "12  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "13  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "14  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "15  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "16  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "17  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "18  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "19  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "20  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "21  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "22  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "23  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "24  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "25  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "26  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "27  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "28  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "29  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  \n",
              "30  # -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-de5936f4-67b1-4560-ad50-e377aeb2dd9a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>repo_path</th>\n",
              "      <th>Source_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/1.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Wed F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/2.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/4.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/5.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/6.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/7.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/8.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/9.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/10.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/11.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/12.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/13.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/14.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/15.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/16.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/17.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/18.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/19.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/20.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/21.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/22.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/23.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/24.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/25.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/26.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/27.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/28.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/29.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/30.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/31.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>/content/gdrive/MyDrive/Dataset/32.py</td>\n",
              "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\nCreated on Thu F...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de5936f4-67b1-4560-ad50-e377aeb2dd9a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-de5936f4-67b1-4560-ad50-e377aeb2dd9a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-de5936f4-67b1-4560-ad50-e377aeb2dd9a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now pre process the daataset\n",
        "\"\"\"\n",
        "1) we create the docstring and function code pair\n",
        "2) extarct function name, function dcstring extarcted\n",
        "3) tokenize docstring and function code\n",
        "\"\"\"\n",
        "def tokenize_docstring(text):\n",
        "    \"\"\"This is the function to tokenize the text\"\"\"\n",
        "\n",
        "    # first wr parition the whole code\n",
        "    before, keyword, after = text.partition(':')\n",
        "    before, keyword, after = before.partition('@param')\n",
        "    before, keyword, after = before.partition('param')\n",
        "    before, keyword, after = before.partition('@brief')\n",
        "\n",
        "    # all the words after will be tokenize\n",
        "    if(after):\n",
        "        words = RegexpTokenizer(r'[a-zA-Z0-9]+').tokenize(after)\n",
        "    else:\n",
        "    # all the docstring is tokenize\n",
        "        before, keyword, after = before.partition('@')\n",
        "        words = RegexpTokenizer(r'[a-zA-Z0-9]+').tokenize(before)\n",
        "\n",
        "    # Every text is converted into lower case\n",
        "    new_words= [word.lower() for word in words if word.isalnum()]\n",
        "\n",
        "    return new_words"
      ],
      "metadata": {
        "id": "cpLLCmozV9Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_code(text):\n",
        "    \"\"\"This is the function to tokenize the function code\"\"\"\n",
        "\n",
        "    # fioorstly we remove all the unnecessary details such as decorators and function signature untill a function is found\n",
        "    keyword = 'def '\n",
        "    # partition the text after the 'def' afetr funding the function\n",
        "    before, keyword, after = text.partition(keyword)\n",
        "    # tokenize the function code\n",
        "    words = RegexpTokenizer(r'[a-zA-Z0-9]+').tokenize(after)\n",
        "\n",
        "    # lower casing the function code and remove the single alphabate variable\n",
        "    new_words= [word.lower() for word in words if (word.isalpha() and len(word)>1) or (word.isnumeric())]\n",
        "    return new_words"
      ],
      "metadata": {
        "id": "Sv34OaSuXi86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_function_docstring_pairs(blob):\n",
        "\n",
        "    \"\"\"This is the function to create a list for function and docstring pair\"\"\"\n",
        "    pairs = []\n",
        "    try:\n",
        "        # by using ast module we create AST structure of the python code\n",
        "        module = ast.parse(blob)\n",
        "        # get the class name using ast module\n",
        "        classes = [node for node in module.body if isinstance(node, ast.ClassDef)]\n",
        "        # get all the funtion name available in a python code\n",
        "        functions = [node for node in module.body if isinstance(node, ast.FunctionDef)]\n",
        "        # run through all the classes to find the result for\n",
        "        for _class in classes:\n",
        "            functions.extend([node for node in _class.body if isinstance(node, ast.FunctionDef)])\n",
        "        # Now iterate through all the function to get the original function without comment and the etarct the docstring for further analysis\n",
        "        for f in functions:\n",
        "            # Conver the function to get the AST tree\n",
        "            source = astor.to_source(f)\n",
        "            # get the docstring from the function if available\n",
        "            docstring = ast.get_docstring(f) if ast.get_docstring(f) else ''\n",
        "            # now in the function remove the docstring and the comment\n",
        "            function = source.replace(ast.get_docstring(f, clean=False), '') if docstring else source\n",
        "\n",
        "            # Extracts function name, line number of the function in the source code, original function, function tokens and docstring tokens\n",
        "            pairs.append((f.name,\n",
        "                          f.lineno,\n",
        "                          source,\n",
        "                          ' '.join(tokenize_code(function)),\n",
        "                          ' '.join(tokenize_docstring(docstring.split('\\n\\n')[0]))\n",
        "                         ))\n",
        "    except (AssertionError, MemoryError, SyntaxError, UnicodeEncodeError):\n",
        "        pass\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "Yp4ka1BSYU46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_function_docstring_pairs_list(blob_list):\n",
        "    \"\"\"apply the function `get_function_docstring_pairs` on a list of blobs\"\"\"\n",
        "    return [get_function_docstring_pairs(b) for b in blob_list]"
      ],
      "metadata": {
        "id": "uvWgmpDlYwwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_doc = get_function_docstring_pairs_list(df.Source_code.tolist())"
      ],
      "metadata": {
        "id": "_Di4ZwAtYpub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(func_doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsK226V2YrwM",
        "outputId": "c6e3f157-03d4-463f-aeb6-076e718c2a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['pairs'] = func_doc"
      ],
      "metadata": {
        "id": "hTYsNbjVY0_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.set_index(['repo_path'])['pairs'].apply(pd.Series).stack()\n",
        "df = df.reset_index()\n",
        "df.columns = ['repo_path', '_', 'pair']\n",
        "\n",
        "# now separate the function and docstring pair\n",
        "df['f_name'] = df['pair'].apply(lambda p: p[0])\n",
        "df['line_number'] = df['pair'].apply(lambda p: p[1])\n",
        "df['original'] = df['pair'].apply(lambda p: p[2])\n",
        "df['f_tokens'] = df['pair'].apply(lambda p: p[3])\n",
        "df['d_tokens'] = df['pair'].apply(lambda p: p[4])"
      ],
      "metadata": {
        "id": "ZWEza0cUbvj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now duplicate all the duplicates\n",
        "before = len(df)\n",
        "df = df.drop_duplicates(['f_tokens'])\n",
        "after = len(df)\n",
        "print(f'found and reove {before - after:,} duplicate rows')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "nN7YxIP-b_0H",
        "outputId": "690c07bd-5541-4da2-c58e-1d98014e8c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d947b5d2244b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# now duplicate all the duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbefore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mafter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'found and reove {before - after:,} duplicate rows'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now separate function with and without docstring from the dataset\n",
        "# get the length of the docstring\n",
        "def listlen(x):\n",
        "    if not isinstance(x, list):\n",
        "        return 0\n",
        "    return len(x)\n",
        "with_= df[df.d_tokens.str.split().apply(listlen) >= 3]\n",
        "without_ = df[df.d_tokens.str.split().apply(listlen) < 3]\n",
        "print('Function with docstring',len(with_))\n",
        "print('Funtion without docstring',len(without_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RMMPOyqckxc",
        "outputId": "2c00872f-a4e8-4dc0-c815-3f4549141df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function with docstring 42\n",
            "Funtion without docstring 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final dataset\n",
        "with_.to_csv('/content/gdrive/MyDrive/SCE_dataset/processed_full.csv')"
      ],
      "metadata": {
        "id": "Z7RMkZAEc_B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, valid = train_test_split(with_, train_size=0.7, random_state=8081)\n",
        "train, test = train_test_split(train, train_size=0.9, random_state=8081)"
      ],
      "metadata": {
        "id": "TvLP9vX51tSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_csv(\"/content/gdrive/MyDrive/SCE_dataset/train.csv\")\n",
        "valid.to_csv(\"/content/gdrive/MyDrive/SCE_dataset/validation.csv\")\n",
        "test.to_csv(\"/content/gdrive/MyDrive/SCE_dataset/test.csv\")"
      ],
      "metadata": {
        "id": "os20UKDy157k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Docstring to a 768 dimension vector"
      ],
      "metadata": {
        "id": "hE9VwMjs2Drv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now first install all the required libraries\n",
        "# the albert transformer is trained over tensorflow 1.15.0\n",
        "# we install tensorflow, tranformer, sentencepiece libraries\n",
        "!pip install transformers\n",
        "!pip install albert-tensorflow\n",
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxEMRS062IMR",
        "outputId": "7cdc8c7f-ac2a-4da5-ef83-7501008c1be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 40.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 43.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n",
            "Collecting albert-tensorflow\n",
            "  Downloading albert_tensorflow-1.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from albert-tensorflow) (1.15.0)\n",
            "Installing collected packages: albert-tensorflow\n",
            "Successfully installed albert-tensorflow-1.1\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 12.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15.0"
      ],
      "metadata": {
        "id": "LwPDMRCk2MuZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9ee102d5-0765-4f35-9812-6f0dbfd3e53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 27 kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.44.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.21.5)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 37.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.14.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=7bf59d50193f60a5d9ff9f0baa4c06cec19b29fb68cf0a3c40e3288c79a486e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZgPRULLIjSH",
        "outputId": "f4b8bd2e-2367-419c-fe88-8580d320b050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/gdrive/MyDrive/Albert model/albert-master/albert-master/create_pretraining_data.py\" \\\n",
        "  --input_file  \"/content/gdrive/MyDrive/SCE_dataset/docstrings.txt\" \\\n",
        "  --output_file  \"/content/gdrive/MyDrive/Albert_model_output_1/tf_example\" \\\n",
        "  --vocab_file  \"/content/gdrive/MyDrive/Albert model/albert_base_3/assets/30k-clean.vocab\" \\\n",
        "  --spm_model_file \"/content/gdrive/MyDrive/Albert model/albert_base_3/assets/30k-clean.model\" \\\n",
        "  --do_lower_case True \\\n",
        "  --max_seq_length 50 \\\n",
        "  --max_predictions_per_seq 8 \\\n",
        "  --random_seed 12345 \\\n",
        "  --dupe_factor 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "har5QIYQIofK",
        "outputId": "66e68460-81d6-4378-9c24-c6174ef1fde6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/albert/tokenization.py:240: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0410 20:52:09.972674 140429856573312 module_wrapper.py:139] From /usr/local/lib/python3.7/dist-packages/albert/tokenization.py:240: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:loading sentence piece model\n",
            "I0410 20:52:09.972921 140429856573312 tokenization.py:240] loading sentence piece model\n",
            "INFO:tensorflow:*** Reading from input files ***\n",
            "I0410 20:52:10.812790 140429856573312 create_pretraining_data.py:631] *** Reading from input files ***\n",
            "INFO:tensorflow:  /content/gdrive/MyDrive/SCE_dataset/docstrings.txt\n",
            "I0410 20:52:10.813139 140429856573312 create_pretraining_data.py:633]   /content/gdrive/MyDrive/SCE_dataset/docstrings.txt\n",
            "INFO:tensorflow:number of instances: 31\n",
            "I0410 20:52:11.222163 140429856573312 create_pretraining_data.py:641] number of instances: 31\n",
            "INFO:tensorflow:*** Writing to output files ***\n",
            "I0410 20:52:11.222572 140429856573312 create_pretraining_data.py:644] *** Writing to output files ***\n",
            "INFO:tensorflow:  /content/gdrive/MyDrive/Albert_model_output_1/tf_example\n",
            "I0410 20:52:11.222722 140429856573312 create_pretraining_data.py:646]   /content/gdrive/MyDrive/Albert_model_output_1/tf_example\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.649271 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁in ▁python ▁given ▁a ▁pattern ▁and [MASK] [MASK] [MASK] [MASK] ▁find ▁if ▁ str ▁follows ▁the ▁same [MASK] ▁here ▁follow ▁means ▁a ▁full ▁match ▁such ▁that ▁there ▁is [MASK] ▁bij ection ▁between [SEP] ▁will ▁split ▁the ▁string ▁up ▁into ▁all ▁the ▁values ▁separated [MASK] [MASK] ▁se par ator [SEP]\n",
            "I0410 20:52:11.649563 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁in ▁python ▁given ▁a ▁pattern ▁and [MASK] [MASK] [MASK] [MASK] ▁find ▁if ▁ str ▁follows ▁the ▁same [MASK] ▁here ▁follow ▁means ▁a ▁full ▁match ▁such ▁that ▁there ▁is [MASK] ▁bij ection ▁between [SEP] ▁will ▁split ▁the ▁string ▁up ▁into ▁all ▁the ▁values ▁separated [MASK] [MASK] ▁se par ator [SEP]\n",
            "INFO:tensorflow:input_ids: 2 19 20059 504 21 3732 17 4 4 4 4 477 100 13 9729 2415 14 205 4 235 1740 1108 21 503 730 145 30 80 25 4 24093 20842 128 3 129 2132 14 3724 71 77 65 14 4070 4196 4 4 1353 3574 3457 3\n",
            "I0410 20:52:11.649755 140429856573312 create_pretraining_data.py:200] input_ids: 2 19 20059 504 21 3732 17 4 4 4 4 477 100 13 9729 2415 14 205 4 235 1740 1108 21 503 730 145 30 80 25 4 24093 20842 128 3 129 2132 14 3724 71 77 65 14 4070 4196 4 4 1353 3574 3457 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.649929 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.650117 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            "I0410 20:52:11.650347 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            "INFO:tensorflow:masked_lm_positions: 7 8 9 10 18 29 44 45\n",
            "I0410 20:52:11.650523 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 7 8 9 10 18 29 44 45\n",
            "INFO:tensorflow:masked_lm_ids: 21 3724 13 9729 3732 21 34 14\n",
            "I0410 20:52:11.650666 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 21 3724 13 9729 3732 21 34 14\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.650841 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.650983 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.651557 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁ str ▁find ▁if ▁ str [MASK] ▁the [MASK] ▁pattern ▁here ▁follow ▁means ▁a ▁full ▁match ▁such ▁unsettled ▁there ▁is ▁a ▁bij ection ▁between [SEP] ▁split ▁the ▁string ▁up ▁into ▁all ▁the ▁values ▁separated ▁by [MASK] [MASK] [MASK] [MASK] ▁pure ▁implementation ▁of ▁sentinel ▁linear ▁search ▁algorithm ▁in ▁python [SEP]\n",
            "I0410 20:52:11.651751 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁ str ▁find ▁if ▁ str [MASK] ▁the [MASK] ▁pattern ▁here ▁follow ▁means ▁a ▁full ▁match ▁such ▁unsettled ▁there ▁is ▁a ▁bij ection ▁between [SEP] ▁split ▁the ▁string ▁up ▁into ▁all ▁the ▁values ▁separated ▁by [MASK] [MASK] [MASK] [MASK] ▁pure ▁implementation ▁of ▁sentinel ▁linear ▁search ▁algorithm ▁in ▁python [SEP]\n",
            "INFO:tensorflow:input_ids: 2 13 9729 477 100 13 9729 4 14 4 3732 235 1740 1108 21 503 730 145 29284 80 25 21 24093 20842 128 3 2132 14 3724 71 77 65 14 4070 4196 34 4 4 4 4 4267 6123 16 17117 6745 2122 9083 19 20059 3\n",
            "I0410 20:52:11.651935 140429856573312 create_pretraining_data.py:200] input_ids: 2 13 9729 477 100 13 9729 4 14 4 3732 235 1740 1108 21 503 730 145 29284 80 25 21 24093 20842 128 3 2132 14 3724 71 77 65 14 4070 4196 34 4 4 4 4 4267 6123 16 17117 6745 2122 9083 19 20059 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.652107 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.652263 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.652430 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 7 9 18 27 36 37 38 39\n",
            "I0410 20:52:11.652564 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 7 9 18 27 36 37 38 39\n",
            "INFO:tensorflow:masked_lm_ids: 2415 205 30 14 14 1353 3574 3457\n",
            "I0410 20:52:11.652705 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 2415 205 30 14 14 1353 3574 3457\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.652846 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.652976 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.653431 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁two ▁strings ▁are ▁is omorphic ▁if ▁the ▁characters ▁in ▁ s ▁can ▁be ▁replaced ▁to ▁get [MASK] ▁clinic ▁all [MASK] [MASK] ▁of ▁a [MASK] [MASK] ▁be ▁replaced ▁with ▁another ▁character ▁while ▁preserving ▁the ▁order ▁of ▁characters ▁no ▁two ▁characters [SEP] ▁pure ▁implementation ▁clump ▁bubble ▁sort ▁algorithm ▁in ▁python [SEP]\n",
            "I0410 20:52:11.653595 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁two ▁strings ▁are ▁is omorphic ▁if ▁the ▁characters ▁in ▁ s ▁can ▁be ▁replaced ▁to ▁get [MASK] ▁clinic ▁all [MASK] [MASK] ▁of ▁a [MASK] [MASK] ▁be ▁replaced ▁with ▁another ▁character ▁while ▁preserving ▁the ▁order ▁of ▁characters ▁no ▁two ▁characters [SEP] ▁pure ▁implementation ▁clump ▁bubble ▁sort ▁algorithm ▁in ▁python [SEP]\n",
            "INFO:tensorflow:input_ids: 2 81 7887 50 25 21282 100 14 1766 19 13 18 92 44 1141 20 164 4 6729 65 4 4 16 21 4 4 44 1141 29 226 925 133 17936 14 389 16 1766 90 81 1766 3 4267 6123 23361 10937 2058 9083 19 20059 3\n",
            "I0410 20:52:11.653815 140429856573312 create_pretraining_data.py:200] input_ids: 2 81 7887 50 25 21282 100 14 1766 19 13 18 92 44 1141 20 164 4 6729 65 4 4 16 21 4 4 44 1141 29 226 925 133 17936 14 389 16 1766 90 81 1766 3 4267 6123 23361 10937 2058 9083 19 20059 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.653992 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.654207 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.654380 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 17 18 19 20 21 24 25 43\n",
            "I0410 20:52:11.654537 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 17 18 19 20 21 24 25 43\n",
            "INFO:tensorflow:masked_lm_ids: 13 38 65 12933 18 925 491 16\n",
            "I0410 20:52:11.654702 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 13 38 65 12933 18 925 491 16\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.654843 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.654997 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.655487 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁will ▁split ▁the ▁string ▁up ▁into ▁all ▁the ▁values ▁separated ▁by ▁the ▁se par ator [MASK] [MASK] ▁of ▁sentinel ▁linear ▁prompting [MASK] ▁in ▁python [SEP] ▁find ▁if ▁ str ▁follows ▁the ▁same ▁pattern [MASK] [MASK] [MASK] ▁a ▁full ▁match ▁such ▁that ▁there ▁is ▁a ▁bij ection ▁between [MASK] [SEP]\n",
            "I0410 20:52:11.655657 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁will ▁split ▁the ▁string ▁up ▁into ▁all ▁the ▁values ▁separated ▁by ▁the ▁se par ator [MASK] [MASK] ▁of ▁sentinel ▁linear ▁prompting [MASK] ▁in ▁python [SEP] ▁find ▁if ▁ str ▁follows ▁the ▁same ▁pattern [MASK] [MASK] [MASK] ▁a ▁full ▁match ▁such ▁that ▁there ▁is ▁a ▁bij ection ▁between [MASK] [SEP]\n",
            "INFO:tensorflow:input_ids: 2 129 2132 14 3724 71 77 65 14 4070 4196 34 14 1353 3574 3457 4 4 16 17117 6745 18332 4 19 20059 3 477 100 13 9729 2415 14 205 3732 4 4 4 21 503 730 145 30 80 25 21 24093 20842 128 4 3\n",
            "I0410 20:52:11.655838 140429856573312 create_pretraining_data.py:200] input_ids: 2 129 2132 14 3724 71 77 65 14 4070 4196 34 14 1353 3574 3457 4 4 16 17117 6745 18332 4 19 20059 3 477 100 13 9729 2415 14 205 3732 4 4 4 21 503 730 145 30 80 25 21 24093 20842 128 4 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.655998 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.656314 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            "I0410 20:52:11.656523 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 16 17 21 22 34 35 36 48\n",
            "I0410 20:52:11.656674 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 16 17 21 22 34 35 36 48\n",
            "INFO:tensorflow:masked_lm_ids: 4267 6123 2122 9083 235 1740 1108 21\n",
            "I0410 20:52:11.656817 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 4267 6123 2122 9083 235 1740 1108 21\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.656961 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0410 20:52:11.657142 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.657785 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁pure ▁implementation [MASK] [MASK] ▁sort ▁in ▁python ▁this ▁function ▁performs ▁gau s sian ▁elimination ▁method ▁function ▁to [MASK] [MASK] [MASK] ▁using ▁pseudo ▁random ▁numbers ▁returns ▁the ▁image ▁with ▁corners ▁identified [MASK] [MASK] ▁path [SEP] ▁perform ▁an ▁insertion ▁sort ▁for ▁given ▁ n ▁elements ▁of ▁array [SEP]\n",
            "I0410 20:52:11.657963 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁pure ▁implementation [MASK] [MASK] ▁sort ▁in ▁python ▁this ▁function ▁performs ▁gau s sian ▁elimination ▁method ▁function ▁to [MASK] [MASK] [MASK] ▁using ▁pseudo ▁random ▁numbers ▁returns ▁the ▁image ▁with ▁corners ▁identified [MASK] [MASK] ▁path [SEP] ▁perform ▁an ▁insertion ▁sort ▁for ▁given ▁ n ▁elements ▁of ▁array [SEP]\n",
            "INFO:tensorflow:input_ids: 2 4267 6123 4 4 2058 19 20059 48 1990 11563 6540 18 10760 10528 2109 1990 20 4 4 4 568 8452 5477 2116 4815 14 1961 29 8894 2889 4 4 2013 3 2985 40 24245 2058 26 504 13 103 2065 16 7718 3 0 0 0\n",
            "I0410 20:52:11.658170 140429856573312 create_pretraining_data.py:200] input_ids: 2 4267 6123 4 4 2058 19 20059 48 1990 11563 6540 18 10760 10528 2109 1990 20 4 4 4 568 8452 5477 2116 4815 14 1961 29 8894 2889 4 4 2013 3 2985 40 24245 2058 26 504 13 103 2065 16 7718 3 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            "I0410 20:52:11.658340 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            "I0410 20:52:11.658532 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0\n",
            "I0410 20:52:11.658695 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 3 4 18 19 20 31 32 0\n",
            "I0410 20:52:11.658827 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 3 4 18 19 20 31 32 0\n",
            "INFO:tensorflow:masked_lm_ids: 16 18417 121 11435 1854 797 263 0\n",
            "I0410 20:52:11.658965 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 16 18417 121 11435 1854 797 263 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0410 20:52:11.659117 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.659252 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.659706 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁search ▁a [MASK] ▁inside ▁a [MASK] ▁outward [MASK] ▁matrix ▁using ▁function ▁function ▁to ▁find ▁pro cent ual ▁proximity ▁pure ▁implementation ▁of ▁1994 australian ▁algorithm ▁in ▁python [SEP] ▁bread th [MASK] ▁search ▁for ▁each ▁node ▁with ▁given ▁parent ▁scalar ▁mul tu plication ▁of ▁a ▁matrix [SEP]\n",
            "I0410 20:52:11.659868 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁search ▁a [MASK] ▁inside ▁a [MASK] ▁outward [MASK] ▁matrix ▁using ▁function ▁function ▁to ▁find ▁pro cent ual ▁proximity ▁pure ▁implementation ▁of ▁1994 australian ▁algorithm ▁in ▁python [SEP] ▁bread th [MASK] ▁search ▁for ▁each ▁node ▁with ▁given ▁parent ▁scalar ▁mul tu plication ▁of ▁a ▁matrix [SEP]\n",
            "INFO:tensorflow:input_ids: 2 2122 21 4 572 21 4 13711 4 8187 568 1990 1990 20 477 895 5089 6948 10493 4267 6123 16 1530 22867 9083 19 20059 3 5988 96 4 2122 26 206 15421 29 504 4766 28960 6633 2473 20669 16 21 8187 3 0 0 0 0\n",
            "I0410 20:52:11.660087 140429856573312 create_pretraining_data.py:200] input_ids: 2 2122 21 4 572 21 4 13711 4 8187 568 1990 1990 20 477 895 5089 6948 10493 4267 6123 16 1530 22867 9083 19 20059 3 5988 96 4 2122 26 206 15421 29 504 4766 28960 6633 2473 20669 16 21 8187 3 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
            "I0410 20:52:11.660315 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
            "I0410 20:52:11.660566 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0\n",
            "I0410 20:52:11.660739 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 3 6 7 8 22 23 30 0\n",
            "I0410 20:52:11.660871 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 3 6 7 8 22 23 30 0\n",
            "INFO:tensorflow:masked_lm_ids: 1246 13 103 9547 10937 2058 64 0\n",
            "I0410 20:52:11.689901 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 1246 13 103 9547 10937 2058 64 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0410 20:52:11.690119 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.690327 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.691159 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] omorphic ▁if ▁the ▁characters ▁in ▁ s ▁prospective ▁be ▁replaced ▁to ▁get [MASK] ▁seize ▁all ▁occurrence s ▁of ▁a ▁character ▁must ▁be [MASK] ▁with ▁another [MASK] ▁while ▁knitting ▁the ▁order ▁of [MASK] ▁no ▁two ▁characters ▁may ▁map ▁to [SEP] ▁returns ▁the ▁image ▁with ▁corners ▁identified ▁im g ▁path [SEP]\n",
            "I0410 20:52:11.691375 140429856573312 create_pretraining_data.py:190] tokens: [CLS] omorphic ▁if ▁the ▁characters ▁in ▁ s ▁prospective ▁be ▁replaced ▁to ▁get [MASK] ▁seize ▁all ▁occurrence s ▁of ▁a ▁character ▁must ▁be [MASK] ▁with ▁another [MASK] ▁while ▁knitting ▁the ▁order ▁of [MASK] ▁no ▁two ▁characters ▁may ▁map ▁to [SEP] ▁returns ▁the ▁image ▁with ▁corners ▁identified ▁im g ▁path [SEP]\n",
            "INFO:tensorflow:input_ids: 2 21282 100 14 1766 19 13 18 20499 44 1141 20 164 4 16073 65 12933 18 16 21 925 491 44 4 29 226 4 133 27731 14 389 16 4 90 81 1766 123 2942 20 3 4815 14 1961 29 8894 2889 797 263 2013 3\n",
            "I0410 20:52:11.691657 140429856573312 create_pretraining_data.py:200] input_ids: 2 21282 100 14 1766 19 13 18 20499 44 1141 20 164 4 16073 65 12933 18 16 21 925 491 44 4 29 226 4 133 27731 14 389 16 4 90 81 1766 123 2942 20 3 4815 14 1961 29 8894 2889 797 263 2013 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.691864 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.692060 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            "I0410 20:52:11.692257 140429856573312 create_pretraining_data.py:200] token_boundary: 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 8 12 13 14 23 26 28 32\n",
            "I0410 20:52:11.692445 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 8 12 13 14 23 26 28 32\n",
            "INFO:tensorflow:masked_lm_ids: 92 164 13 38 1141 925 17936 1766\n",
            "I0410 20:52:11.692631 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 92 164 13 38 1141 925 17936 1766\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.692816 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.692960 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.693440 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁scalar ▁mul tu plication ▁of [MASK] [MASK] [SEP] ▁search ▁a [MASK] ▁inside [MASK] [MASK] [MASK] ▁dimension ▁matrix ▁using ▁function ▁function ▁to ▁find ▁pro cent ual ▁proximity [MASK] ▁implementation ▁of ▁bubble ▁sort ▁algorithm ▁in ▁python ▁calculate ▁the ▁waiting ▁times [MASK] ▁a ▁list ▁of ▁processes ▁that ▁have ▁a ▁specified ▁duration [SEP]\n",
            "I0410 20:52:11.693645 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁scalar ▁mul tu plication ▁of [MASK] [MASK] [SEP] ▁search ▁a [MASK] ▁inside [MASK] [MASK] [MASK] ▁dimension ▁matrix ▁using ▁function ▁function ▁to ▁find ▁pro cent ual ▁proximity [MASK] ▁implementation ▁of ▁bubble ▁sort ▁algorithm ▁in ▁python ▁calculate ▁the ▁waiting ▁times [MASK] ▁a ▁list ▁of ▁processes ▁that ▁have ▁a ▁specified ▁duration [SEP]\n",
            "INFO:tensorflow:input_ids: 2 28960 6633 2473 20669 16 4 4 3 2122 21 4 572 4 4 4 9547 8187 568 1990 1990 20 477 895 5089 6948 10493 4 6123 16 10937 2058 9083 19 20059 18469 14 1672 436 4 21 968 16 5102 30 57 21 9931 9403 3\n",
            "I0410 20:52:11.693822 140429856573312 create_pretraining_data.py:200] input_ids: 2 28960 6633 2473 20669 16 4 4 3 2122 21 4 572 4 4 4 9547 8187 568 1990 1990 20 477 895 5089 6948 10493 4 6123 16 10937 2058 9083 19 20059 18469 14 1672 436 4 21 968 16 5102 30 57 21 9931 9403 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.693982 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.694137 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.694289 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 6 7 11 13 14 15 27 39\n",
            "I0410 20:52:11.694431 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 6 7 11 13 14 15 27 39\n",
            "INFO:tensorflow:masked_lm_ids: 21 8187 1246 21 13 103 4267 16\n",
            "I0410 20:52:11.694636 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 21 8187 1246 21 13 103 4267 16\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.694874 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0410 20:52:11.695028 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.695508 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁function ▁to ▁find ▁a ▁factor ial ▁of ▁a ▁ n ▁number [SEP] ▁matrix ▁multiplication ▁using [MASK] ▁check ▁if ▁all [MASK] [MASK] [MASK] ▁string ▁is ▁unique ▁or ▁not ▁two ▁matrix ▁sub str action ▁function ▁using ▁list ▁function [MASK] [MASK] ▁a ▁decimal ▁to ▁binary ▁fu c nc tion ▁to [MASK] [SEP]\n",
            "I0410 20:52:11.695692 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁function ▁to ▁find ▁a ▁factor ial ▁of ▁a ▁ n ▁number [SEP] ▁matrix ▁multiplication ▁using [MASK] ▁check ▁if ▁all [MASK] [MASK] [MASK] ▁string ▁is ▁unique ▁or ▁not ▁two ▁matrix ▁sub str action ▁function ▁using ▁list ▁function [MASK] [MASK] ▁a ▁decimal ▁to ▁binary ▁fu c nc tion ▁to [MASK] [SEP]\n",
            "INFO:tensorflow:input_ids: 2 1990 20 477 21 3932 2815 16 21 13 103 234 3 8187 25432 568 4 2631 100 65 4 4 4 3724 25 2619 54 52 81 8187 972 9729 8645 1990 568 968 1990 4 4 21 26380 20 14171 2916 150 6897 3309 20 4 3\n",
            "I0410 20:52:11.695873 140429856573312 create_pretraining_data.py:200] input_ids: 2 1990 20 477 21 3932 2815 16 21 13 103 234 3 8187 25432 568 4 2631 100 65 4 4 4 3724 25 2619 54 52 81 8187 972 9729 8645 1990 568 968 1990 4 4 21 26380 20 14171 2916 150 6897 3309 20 4 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.696050 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.696207 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1\n",
            "I0410 20:52:11.696436 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 1 16 20 21 22 37 38 48\n",
            "I0410 20:52:11.696575 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 1 16 20 21 22 37 38 48\n",
            "INFO:tensorflow:masked_lm_ids: 1990 968 1766 19 14 20 8406 9267\n",
            "I0410 20:52:11.696743 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 1990 968 1766 19 14 20 8406 9267\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.696882 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.697097 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.697629 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁in verse ▁of ▁a ▁matrix ▁a ▁with [MASK] [MASK] ▁dimension ▁pure ▁implementation ▁of [MASK] ▁bog o sort ▁algorithm ▁in ▁python [SEP] ▁function [MASK] ▁en crypt ▁text ▁using ▁pseudo [MASK] ▁numbers [MASK] ▁of ▁the ▁lev en sh te in ▁distance ▁in ▁python [SEP]\n",
            "I0410 20:52:11.697833 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁in verse ▁of ▁a ▁matrix ▁a ▁with [MASK] [MASK] ▁dimension ▁pure ▁implementation ▁of [MASK] ▁bog o sort ▁algorithm ▁in ▁python [SEP] ▁function [MASK] ▁en crypt ▁text ▁using ▁pseudo [MASK] ▁numbers [MASK] ▁of ▁the ▁lev en sh te in ▁distance ▁in ▁python [SEP]\n",
            "INFO:tensorflow:input_ids: 2 19 9453 16 21 8187 21 29 4 4 9547 4267 6123 16 4 6509 111 22843 9083 19 20059 3 1990 4 1957 11435 1854 568 8452 4 2116 4 16 14 9183 219 1635 591 108 1583 19 20059 3 0 0 0 0 0 0 0\n",
            "I0410 20:52:11.698012 140429856573312 create_pretraining_data.py:200] input_ids: 2 19 9453 16 21 8187 21 29 4 4 9547 4267 6123 16 4 6509 111 22843 9083 19 20059 3 1990 4 1957 11435 1854 568 8452 4 2116 4 16 14 9183 219 1635 591 108 1583 19 20059 3 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
            "I0410 20:52:11.698186 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
            "I0410 20:52:11.698351 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            "I0410 20:52:11.698534 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 8 9 14 23 29 31 0 0\n",
            "I0410 20:52:11.698667 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 8 9 14 23 29 31 0 0\n",
            "INFO:tensorflow:masked_lm_ids: 13 103 14 20 5477 6123 0 0\n",
            "I0410 20:52:11.698841 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 13 103 14 20 5477 6123 0 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0\n",
            "I0410 20:52:11.698987 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.699152 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.699694 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁search [MASK] [MASK] ▁inside ▁a ▁ n [MASK] ▁matrix ▁using ▁function ▁function [MASK] [MASK] ▁pro cent ual ▁proximity ▁pure ▁implementation [MASK] ▁bubble ▁sort ▁algorithm ▁in ▁python ▁calculate ▁the [MASK] ▁times ▁of ▁a ▁list ▁of ▁processes ▁that ▁have ▁a ▁specified ▁duration [SEP] ▁scalar ▁mul tu plication ▁of ▁a ▁matrix [SEP]\n",
            "I0410 20:52:11.699908 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁search [MASK] [MASK] ▁inside ▁a ▁ n [MASK] ▁matrix ▁using ▁function ▁function [MASK] [MASK] ▁pro cent ual ▁proximity ▁pure ▁implementation [MASK] ▁bubble ▁sort ▁algorithm ▁in ▁python ▁calculate ▁the [MASK] ▁times ▁of ▁a ▁list ▁of ▁processes ▁that ▁have ▁a ▁specified ▁duration [SEP] ▁scalar ▁mul tu plication ▁of ▁a ▁matrix [SEP]\n",
            "INFO:tensorflow:input_ids: 2 2122 4 4 572 21 13 103 4 8187 568 1990 1990 4 4 895 5089 6948 10493 4267 6123 4 10937 2058 9083 19 20059 18469 14 4 436 16 21 968 16 5102 30 57 21 9931 9403 3 28960 6633 2473 20669 16 21 8187 3\n",
            "I0410 20:52:11.700172 140429856573312 create_pretraining_data.py:200] input_ids: 2 2122 4 4 572 21 13 103 4 8187 568 1990 1990 4 4 895 5089 6948 10493 4267 6123 4 10937 2058 9083 19 20059 18469 14 4 436 16 21 968 16 5102 30 57 21 9931 9403 3 28960 6633 2473 20669 16 21 8187 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.700352 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.700585 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            "I0410 20:52:11.700816 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 2 3 8 13 14 21 29 37\n",
            "I0410 20:52:11.701009 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 2 3 8 13 14 21 29 37\n",
            "INFO:tensorflow:masked_lm_ids: 21 1246 9547 20 477 16 1672 57\n",
            "I0410 20:52:11.701226 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 21 1246 9547 20 477 16 1672 57\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.701488 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.701730 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.702204 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] [MASK] [MASK] ▁of ▁bubble ▁sort ▁algorithm ▁in ▁python ▁calculate ▁the ▁waiting ▁times ▁of ▁a ▁list ▁of ▁processes ▁that ▁have ▁a ▁specified ▁duration [SEP] ▁scalar [MASK] tu plication ▁of ▁a ▁matrix ▁search [MASK] [MASK] [MASK] ▁a ▁ n ▁dimension ▁matrix ▁using ▁function ▁function ▁to ▁find ▁pro cent ual ▁proximity [SEP]\n",
            "I0410 20:52:11.702413 140429856573312 create_pretraining_data.py:190] tokens: [CLS] [MASK] [MASK] ▁of ▁bubble ▁sort ▁algorithm ▁in ▁python ▁calculate ▁the ▁waiting ▁times ▁of ▁a ▁list ▁of ▁processes ▁that ▁have ▁a ▁specified ▁duration [SEP] ▁scalar [MASK] tu plication ▁of ▁a ▁matrix ▁search [MASK] [MASK] [MASK] ▁a ▁ n ▁dimension ▁matrix ▁using ▁function ▁function ▁to ▁find ▁pro cent ual ▁proximity [SEP]\n",
            "INFO:tensorflow:input_ids: 2 4 4 16 10937 2058 9083 19 20059 18469 14 1672 436 16 21 968 16 5102 30 57 21 9931 9403 3 28960 4 2473 20669 16 21 8187 2122 4 4 4 21 13 103 9547 8187 568 1990 1990 20 477 895 5089 6948 10493 3\n",
            "I0410 20:52:11.702603 140429856573312 create_pretraining_data.py:200] input_ids: 2 4 4 16 10937 2058 9083 19 20059 18469 14 1672 436 16 21 968 16 5102 30 57 21 9931 9403 3 28960 4 2473 20669 16 21 8187 2122 4 4 4 21 13 103 9547 8187 568 1990 1990 20 477 895 5089 6948 10493 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.702811 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.791404 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            "I0410 20:52:11.791750 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 1 2 25 26 27 32 33 34\n",
            "I0410 20:52:11.791968 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 1 2 25 26 27 32 33 34\n",
            "INFO:tensorflow:masked_lm_ids: 4267 6123 6633 2473 20669 21 1246 572\n",
            "I0410 20:52:11.792171 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 4267 6123 6633 2473 20669 21 1246 572\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.792369 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.792613 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.793427 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁if ▁ str ▁follows ▁the ▁same ▁pattern ▁here ▁follow ▁means ▁a ▁full [MASK] ▁such ▁that ▁there ▁is ▁a [MASK] [MASK] ▁between ▁a [MASK] [MASK] [SEP] [MASK] ▁the ▁string ▁up ▁into ▁all ▁the ▁values ▁separated ▁fleeing ▁the ▁se par ator ▁pure ▁implementation ▁of ▁sentinel ▁linear [MASK] ▁algorithm ▁in ▁python [SEP]\n",
            "I0410 20:52:11.793689 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁if ▁ str ▁follows ▁the ▁same ▁pattern ▁here ▁follow ▁means ▁a ▁full [MASK] ▁such ▁that ▁there ▁is ▁a [MASK] [MASK] ▁between ▁a [MASK] [MASK] [SEP] [MASK] ▁the ▁string ▁up ▁into ▁all ▁the ▁values ▁separated ▁fleeing ▁the ▁se par ator ▁pure ▁implementation ▁of ▁sentinel ▁linear [MASK] ▁algorithm ▁in ▁python [SEP]\n",
            "INFO:tensorflow:input_ids: 2 100 13 9729 2415 14 205 3732 235 1740 1108 21 503 4 145 30 80 25 21 4 4 128 21 4 4 3 4 14 3724 71 77 65 14 4070 4196 15624 14 1353 3574 3457 4267 6123 16 17117 6745 4 9083 19 20059 3\n",
            "I0410 20:52:11.793915 140429856573312 create_pretraining_data.py:200] input_ids: 2 100 13 9729 2415 14 205 3732 235 1740 1108 21 503 4 145 30 80 25 21 4 4 128 21 4 4 3 4 14 3724 71 77 65 14 4070 4196 15624 14 1353 3574 3457 4267 6123 16 17117 6745 4 9083 19 20059 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.794155 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.794361 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.794642 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 13 19 20 23 24 26 35 45\n",
            "I0410 20:52:11.794839 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 13 19 20 23 24 26 35 45\n",
            "INFO:tensorflow:masked_lm_ids: 730 24093 20842 1748 19 2132 34 2122\n",
            "I0410 20:52:11.795018 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 730 24093 20842 1748 19 2132 34 2122\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.795202 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.795409 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.796124 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁fu c nc tion ▁to ▁define ▁how ▁kn ap s ack ▁algorithm [MASK] [SEP] ▁regular ▁matrix [MASK] ▁using ▁list ▁check ▁if [MASK] [MASK] [MASK] [MASK] ▁string ▁is ▁unique [MASK] ▁not ▁two ▁matrix ▁sub str action ▁function ▁using ▁list ▁function ▁to ▁convert ▁a ▁decimal ▁to ▁binary [SEP]\n",
            "I0410 20:52:11.796386 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁fu c nc tion ▁to ▁define ▁how ▁kn ap s ack ▁algorithm [MASK] [SEP] ▁regular ▁matrix [MASK] ▁using ▁list ▁check ▁if [MASK] [MASK] [MASK] [MASK] ▁string ▁is ▁unique [MASK] ▁not ▁two ▁matrix ▁sub str action ▁function ▁using ▁list ▁function ▁to ▁convert ▁a ▁decimal ▁to ▁binary [SEP]\n",
            "INFO:tensorflow:input_ids: 2 2916 150 6897 3309 20 9267 184 6209 2552 18 8735 9083 4 3 1290 8187 4 568 968 2631 100 4 4 4 4 3724 25 2619 4 52 81 8187 972 9729 8645 1990 568 968 1990 20 8406 21 26380 20 14171 3 0 0 0\n",
            "I0410 20:52:11.796617 140429856573312 create_pretraining_data.py:200] input_ids: 2 2916 150 6897 3309 20 9267 184 6209 2552 18 8735 9083 4 3 1290 8187 4 568 968 2631 100 4 4 4 4 3724 25 2619 4 52 81 8187 972 9729 8645 1990 568 968 1990 20 8406 21 26380 20 14171 3 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            "I0410 20:52:11.796790 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            "I0410 20:52:11.796954 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            "INFO:tensorflow:token_boundary: 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            "I0410 20:52:11.797169 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
            "INFO:tensorflow:masked_lm_positions: 13 17 22 23 24 25 29 0\n",
            "I0410 20:52:11.797306 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 13 17 22 23 24 25 29 0\n",
            "INFO:tensorflow:masked_lm_ids: 693 25432 65 1766 19 14 54 0\n",
            "I0410 20:52:11.797461 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 693 25432 65 1766 19 14 54 0\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "I0410 20:52:11.797638 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.797778 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.798242 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁function ▁to ▁en crypt ▁text ▁using ▁pseudo ▁random ▁numbers ▁implementation ▁of ▁the ▁lev en sh te in ▁distance [MASK] [MASK] [SEP] [MASK] ▁ireland ▁a ▁with ▁ n ▁dimension ▁pure [MASK] ▁of ▁the ▁bog o sort ▁algorithm [MASK] ▁python ▁bread th ▁first ▁search [MASK] ▁each ▁node ▁with ▁given [MASK] [SEP]\n",
            "I0410 20:52:11.798443 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁function ▁to ▁en crypt ▁text ▁using ▁pseudo ▁random ▁numbers ▁implementation ▁of ▁the ▁lev en sh te in ▁distance [MASK] [MASK] [SEP] [MASK] ▁ireland ▁a ▁with ▁ n ▁dimension ▁pure [MASK] ▁of ▁the ▁bog o sort ▁algorithm [MASK] ▁python ▁bread th ▁first ▁search [MASK] ▁each ▁node ▁with ▁given [MASK] [SEP]\n",
            "INFO:tensorflow:input_ids: 2 1990 20 1957 11435 1854 568 8452 5477 2116 6123 16 14 9183 219 1635 591 108 1583 4 4 3 4 1545 21 29 13 103 9547 4267 4 16 14 6509 111 22843 9083 4 20059 5988 96 64 2122 4 206 15421 29 504 4 3\n",
            "I0410 20:52:11.798632 140429856573312 create_pretraining_data.py:200] input_ids: 2 1990 20 1957 11435 1854 568 8452 5477 2116 6123 16 14 9183 219 1635 591 108 1583 4 4 3 4 1545 21 29 13 103 9547 4267 4 16 14 6509 111 22843 9083 4 20059 5988 96 64 2122 4 206 15421 29 504 4 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.798805 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.798994 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.799160 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 19 20 22 23 30 37 43 48\n",
            "I0410 20:52:11.799294 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 19 20 22 23 30 37 43 48\n",
            "INFO:tensorflow:masked_lm_ids: 19 20059 21 8187 6123 19 26 4766\n",
            "I0410 20:52:11.799461 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 19 20059 21 8187 6123 19 26 4766\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.799603 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0410 20:52:11.799759 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.800312 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁function ▁to ▁create ▁identity ▁matrix ▁for ▁ n ▁dimension [SEP] omorphic ▁two ▁strings ▁extra [MASK] [MASK] ▁if ▁the ▁characters ▁in ▁ s ▁can ▁be ▁sense [MASK] [MASK] ▁ t ▁all ▁occurrence s ▁of ▁a ▁character ▁must ▁be ▁replaced ▁with ▁another ▁character ▁while ▁preserving ▁the ▁order ▁of ▁characters [MASK] [SEP]\n",
            "I0410 20:52:11.800508 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁function ▁to ▁create ▁identity ▁matrix ▁for ▁ n ▁dimension [SEP] omorphic ▁two ▁strings ▁extra [MASK] [MASK] ▁if ▁the ▁characters ▁in ▁ s ▁can ▁be ▁sense [MASK] [MASK] ▁ t ▁all ▁occurrence s ▁of ▁a ▁character ▁must ▁be ▁replaced ▁with ▁another ▁character ▁while ▁preserving ▁the ▁order ▁of ▁characters [MASK] [SEP]\n",
            "INFO:tensorflow:input_ids: 2 1990 20 1600 3270 8187 26 13 103 9547 3 21282 81 7887 2230 4 4 100 14 1766 19 13 18 92 44 1259 4 4 13 38 65 12933 18 16 21 925 491 44 1141 29 226 925 133 17936 14 389 16 1766 4 3\n",
            "I0410 20:52:11.800704 140429856573312 create_pretraining_data.py:200] input_ids: 2 1990 20 1600 3270 8187 26 13 103 9547 3 21282 81 7887 2230 4 4 100 14 1766 19 13 18 92 44 1259 4 4 13 38 65 12933 18 16 21 925 491 44 1141 29 226 925 133 17936 14 389 16 1766 4 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.800873 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.801063 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.801222 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 14 15 16 20 25 26 27 48\n",
            "I0410 20:52:11.801359 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 14 15 16 20 25 26 27 48\n",
            "INFO:tensorflow:masked_lm_ids: 50 25 21282 19 1141 20 164 90\n",
            "I0410 20:52:11.801521 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 50 25 21282 19 1141 20 164 90\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.801717 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0410 20:52:11.801881 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.802662 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] omorphic ▁two ▁strings ▁are ▁is omorphic ▁if ▁the ▁characters ▁in [MASK] zig ▁can [MASK] [MASK] [MASK] ▁get ▁ t ▁all ▁occurrence s [MASK] ▁a ▁character ▁must ▁be ▁replaced ▁with ▁another ▁character ▁while ▁preserving ▁the ▁order ▁of ▁characters ▁no [SEP] [MASK] [MASK] ▁en crypt ▁text ▁using ▁pseudo ▁random ▁numbers [SEP]\n",
            "I0410 20:52:11.802862 140429856573312 create_pretraining_data.py:190] tokens: [CLS] omorphic ▁two ▁strings ▁are ▁is omorphic ▁if ▁the ▁characters ▁in [MASK] zig ▁can [MASK] [MASK] [MASK] ▁get ▁ t ▁all ▁occurrence s [MASK] ▁a ▁character ▁must ▁be ▁replaced ▁with ▁another ▁character ▁while ▁preserving ▁the ▁order ▁of ▁characters ▁no [SEP] [MASK] [MASK] ▁en crypt ▁text ▁using ▁pseudo ▁random ▁numbers [SEP]\n",
            "INFO:tensorflow:input_ids: 2 21282 81 7887 50 25 21282 100 14 1766 19 4 16594 92 4 4 4 164 13 38 65 12933 18 4 21 925 491 44 1141 29 226 925 133 17936 14 389 16 1766 90 3 4 4 1957 11435 1854 568 8452 5477 2116 3\n",
            "I0410 20:52:11.803064 140429856573312 create_pretraining_data.py:200] input_ids: 2 21282 81 7887 50 25 21282 100 14 1766 19 4 16594 92 4 4 4 164 13 38 65 12933 18 4 21 925 491 44 1141 29 226 925 133 17936 14 389 16 1766 90 3 4 4 1957 11435 1854 568 8452 5477 2116 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.803235 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.803408 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            "I0410 20:52:11.803622 140429856573312 create_pretraining_data.py:200] token_boundary: 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 11 12 14 15 16 23 40 41\n",
            "I0410 20:52:11.803758 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 11 12 14 15 16 23 40 41\n",
            "INFO:tensorflow:masked_lm_ids: 13 18 44 1141 20 16 1990 20\n",
            "I0410 20:52:11.803925 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 13 18 44 1141 20 16 1990 20\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.804083 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.804220 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.804792 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁for ▁given ▁ n ▁elements [MASK] [MASK] ▁pure ▁implementation ▁of ▁stump ▁sort ▁in ▁python ▁this ▁function [MASK] ▁gau s sian ▁elimination ▁method [MASK] ▁to [MASK] [MASK] ▁text ▁using ▁pseudo [SEP] ▁returns ▁the ▁image ▁with ▁corners ▁identified ▁im g ▁path ▁function ▁to ▁create ▁identity ▁matrix ▁for ▁ n ▁dimension [SEP]\n",
            "I0410 20:52:11.804969 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁for ▁given ▁ n ▁elements [MASK] [MASK] ▁pure ▁implementation ▁of ▁stump ▁sort ▁in ▁python ▁this ▁function [MASK] ▁gau s sian ▁elimination ▁method [MASK] ▁to [MASK] [MASK] ▁text ▁using ▁pseudo [SEP] ▁returns ▁the ▁image ▁with ▁corners ▁identified ▁im g ▁path ▁function ▁to ▁create ▁identity ▁matrix ▁for ▁ n ▁dimension [SEP]\n",
            "INFO:tensorflow:input_ids: 2 26 504 13 103 2065 4 4 4267 6123 16 15781 2058 19 20059 48 1990 4 6540 18 10760 10528 2109 4 20 4 4 1854 568 8452 3 4815 14 1961 29 8894 2889 797 263 2013 1990 20 1600 3270 8187 26 13 103 9547 3\n",
            "I0410 20:52:11.805182 140429856573312 create_pretraining_data.py:200] input_ids: 2 26 504 13 103 2065 4 4 4267 6123 16 15781 2058 19 20059 48 1990 4 6540 18 10760 10528 2109 4 20 4 4 1854 568 8452 3 4815 14 1961 29 8894 2889 797 263 2013 1990 20 1600 3270 8187 26 13 103 9547 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.805345 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.805555 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            "I0410 20:52:11.805727 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 6 7 11 13 17 23 25 26\n",
            "I0410 20:52:11.893808 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 6 7 11 13 17 23 25 26\n",
            "INFO:tensorflow:masked_lm_ids: 16 7718 18417 19 11563 1990 121 11435\n",
            "I0410 20:52:11.894016 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 16 7718 18417 19 11563 1990 121 11435\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.894241 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0410 20:52:11.894447 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.895195 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁function ▁to ▁en crypt ▁text ▁using ▁pseudo ▁random ▁numbers ▁implementation [MASK] ▁the [MASK] en [MASK] [MASK] [MASK] ▁distance ▁in ▁python [SEP] ▁a ▁matrix ▁a ▁with ▁ n ▁dimension ▁pure ▁implementation ▁of ▁the ▁bog o sort ▁algorithm ▁in ▁python ▁bread th ▁first ▁search [MASK] [MASK] ▁node ▁with ▁given ▁parent [SEP]\n",
            "I0410 20:52:11.895430 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁function ▁to ▁en crypt ▁text ▁using ▁pseudo ▁random ▁numbers ▁implementation [MASK] ▁the [MASK] en [MASK] [MASK] [MASK] ▁distance ▁in ▁python [SEP] ▁a ▁matrix ▁a ▁with ▁ n ▁dimension ▁pure ▁implementation ▁of ▁the ▁bog o sort ▁algorithm ▁in ▁python ▁bread th ▁first ▁search [MASK] [MASK] ▁node ▁with ▁given ▁parent [SEP]\n",
            "INFO:tensorflow:input_ids: 2 1990 20 1957 11435 1854 568 8452 5477 2116 6123 4 14 4 219 4 4 4 1583 19 20059 3 21 8187 21 29 13 103 9547 4267 6123 16 14 6509 111 22843 9083 19 20059 5988 96 64 2122 4 4 15421 29 504 4766 3\n",
            "I0410 20:52:11.895671 140429856573312 create_pretraining_data.py:200] input_ids: 2 1990 20 1957 11435 1854 568 8452 5477 2116 6123 4 14 4 219 4 4 4 1583 19 20059 3 21 8187 21 29 13 103 9547 4267 6123 16 14 6509 111 22843 9083 19 20059 5988 96 64 2122 4 4 15421 29 504 4766 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.895867 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.896088 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.896432 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:masked_lm_positions: 11 13 14 15 16 17 43 44\n",
            "I0410 20:52:11.896619 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 11 13 14 15 16 17 43 44\n",
            "INFO:tensorflow:masked_lm_ids: 16 9183 219 1635 591 108 26 206\n",
            "I0410 20:52:11.897467 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 16 9183 219 1635 591 108 26 206\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.897779 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 0\n",
            "I0410 20:52:11.897998 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "I0410 20:52:11.898996 140429856573312 create_pretraining_data.py:188] *** Example ***\n",
            "INFO:tensorflow:tokens: [CLS] ▁if ▁the ▁characters ▁in ▁ s ▁can ▁be ▁replaced [MASK] [MASK] [MASK] t ▁all ▁occurrence s ▁of ▁a ▁character ▁must ▁be ▁replaced ▁with ▁another ▁character ▁while ▁preserving ▁the ▁order [MASK] [MASK] [MASK] [SEP] ▁will ▁split [MASK] ▁string ▁up ▁into ▁all ▁the ▁values ▁separated ▁by ▁the ▁se par ator [SEP]\n",
            "I0410 20:52:11.899323 140429856573312 create_pretraining_data.py:190] tokens: [CLS] ▁if ▁the ▁characters ▁in ▁ s ▁can ▁be ▁replaced [MASK] [MASK] [MASK] t ▁all ▁occurrence s ▁of ▁a ▁character ▁must ▁be ▁replaced ▁with ▁another ▁character ▁while ▁preserving ▁the ▁order [MASK] [MASK] [MASK] [SEP] ▁will ▁split [MASK] ▁string ▁up ▁into ▁all ▁the ▁values ▁separated ▁by ▁the ▁se par ator [SEP]\n",
            "INFO:tensorflow:input_ids: 2 100 14 1766 19 13 18 92 44 1141 4 4 4 38 65 12933 18 16 21 925 491 44 1141 29 226 925 133 17936 14 389 4 4 4 3 129 2132 4 3724 71 77 65 14 4070 4196 34 14 1353 3574 3457 3\n",
            "I0410 20:52:11.899696 140429856573312 create_pretraining_data.py:200] input_ids: 2 100 14 1766 19 13 18 92 44 1141 4 4 4 38 65 12933 18 16 21 925 491 44 1141 29 226 925 133 17936 14 389 4 4 4 3 129 2132 4 3724 71 77 65 14 4070 4196 34 14 1353 3574 3457 3\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.899998 140429856573312 create_pretraining_data.py:200] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0410 20:52:11.900307 140429856573312 create_pretraining_data.py:200] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "INFO:tensorflow:token_boundary: 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            "I0410 20:52:11.900626 140429856573312 create_pretraining_data.py:200] token_boundary: 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            "INFO:tensorflow:masked_lm_positions: 10 11 12 13 30 31 32 36\n",
            "I0410 20:52:11.900876 140429856573312 create_pretraining_data.py:200] masked_lm_positions: 10 11 12 13 30 31 32 36\n",
            "INFO:tensorflow:masked_lm_ids: 20 164 13 38 16 1766 90 14\n",
            "I0410 20:52:11.901136 140429856573312 create_pretraining_data.py:200] masked_lm_ids: 20 164 13 38 16 1766 90 14\n",
            "INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0410 20:52:11.901431 140429856573312 create_pretraining_data.py:200] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "INFO:tensorflow:next_sentence_labels: 1\n",
            "I0410 20:52:11.901703 140429856573312 create_pretraining_data.py:200] next_sentence_labels: 1\n",
            "INFO:tensorflow:Wrote 31 total instances\n",
            "I0410 20:52:11.914103 140429856573312 create_pretraining_data.py:205] Wrote 31 total instances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/gdrive/MyDrive/Albert model/albert-master/albert-master/run_pretraining.py\" \\\n",
        "    --input_file \"/content/gdrive/MyDrive/Albert_model_output_1/tf_example\" \\\n",
        "    --output_dir \"/content/gdrive/MyDrive/Albert_model_output_1\" \\\n",
        "    --albert_config_file \"/content/gdrive/MyDrive/Albert model/albert_base_3/assets/albert_config.json\" \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --train_batch_size=64 \\\n",
        "    --eval_batch_size=32 \\\n",
        "    --max_seq_length 50 \\\n",
        "    --max_predictions_per_seq 8 \\\n",
        "    --optimizer 'lamb' \\\n",
        "    --learning_rate .00176 \\\n",
        "    --num_train_steps 100 \\\n",
        "    --num_warmup_steps 10 \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y03p5p9kIvVX",
        "outputId": "e3540eb0-be02-407a-c7e9-46a6534b6c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/albert/lamb_optimizer.py:34: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/albert/modeling.py:116: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0410 20:52:19.295110 140400511838080 module_wrapper.py:139] From /usr/local/lib/python3.7/dist-packages/albert/modeling.py:116: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "INFO:tensorflow:*** Input Files ***\n",
            "I0410 20:52:19.760158 140400511838080 run_pretraining.py:484] *** Input Files ***\n",
            "INFO:tensorflow:  /content/gdrive/MyDrive/Albert_model_output_1/tf_example\n",
            "I0410 20:52:19.760502 140400511838080 run_pretraining.py:486]   /content/gdrive/MyDrive/Albert_model_output_1/tf_example\n",
            "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fb11b3d0d40>) includes params argument, but params are not passed to Estimator.\n",
            "W0410 20:52:19.761241 140400511838080 estimator.py:1994] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fb11b3d0d40>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/content/gdrive/MyDrive/Albert_model_output_1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 5000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb11a017210>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
            "I0410 20:52:19.763152 140400511838080 estimator.py:212] Using config: {'_model_dir': '/content/gdrive/MyDrive/Albert_model_output_1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 5000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb11a017210>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "I0410 20:52:19.763568 140400511838080 tpu_context.py:220] _TPUContext: eval_on_tpu True\n",
            "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
            "W0410 20:52:19.763998 140400511838080 tpu_context.py:222] eval_on_tpu ignored because use_tpu is False.\n",
            "INFO:tensorflow:***** Running training *****\n",
            "I0410 20:52:19.764234 140400511838080 run_pretraining.py:527] ***** Running training *****\n",
            "INFO:tensorflow:  Batch size = 64\n",
            "I0410 20:52:19.764487 140400511838080 run_pretraining.py:528]   Batch size = 64\n",
            "INFO:tensorflow:Skipping training since max_steps has already saved.\n",
            "I0410 20:52:21.059531 140400511838080 estimator.py:363] Skipping training since max_steps has already saved.\n",
            "INFO:tensorflow:training_loop marked as finished\n",
            "I0410 20:52:21.059873 140400511838080 error_handling.py:101] training_loop marked as finished\n",
            "INFO:tensorflow:***** Running evaluation *****\n",
            "I0410 20:52:21.060077 140400511838080 run_pretraining.py:537] ***** Running evaluation *****\n",
            "INFO:tensorflow:  Batch size = 32\n",
            "I0410 20:52:21.060228 140400511838080 run_pretraining.py:538]   Batch size = 32\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "W0410 20:52:21.075870 140400511838080 deprecation.py:506] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/Albert model/albert-master/albert-master/run_pretraining.py:448: map_and_batch_with_legacy_function (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch()\n",
            "W0410 20:52:21.093625 140400511838080 deprecation.py:323] From /content/gdrive/MyDrive/Albert model/albert-master/albert-master/run_pretraining.py:448: map_and_batch_with_legacy_function (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch()\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/Albert model/albert-master/albert-master/run_pretraining.py:464: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0410 20:52:21.295117 140400511838080 deprecation.py:323] From /content/gdrive/MyDrive/Albert model/albert-master/albert-master/run_pretraining.py:464: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:<DatasetV1Adapter shapes: {input_ids: (32, 50), input_mask: (32, 50), masked_lm_ids: (32, 8), masked_lm_positions: (32, 8), masked_lm_weights: (32, 8), next_sentence_labels: (32, 1), segment_ids: (32, 50)}, types: {input_ids: tf.int32, input_mask: tf.int32, masked_lm_ids: tf.int32, masked_lm_positions: tf.int32, masked_lm_weights: tf.float32, next_sentence_labels: tf.int32, segment_ids: tf.int32}>\n",
            "I0410 20:52:21.304769 140400511838080 run_pretraining.py:449] <DatasetV1Adapter shapes: {input_ids: (32, 50), input_mask: (32, 50), masked_lm_ids: (32, 8), masked_lm_positions: (32, 8), masked_lm_weights: (32, 8), next_sentence_labels: (32, 1), segment_ids: (32, 50)}, types: {input_ids: tf.int32, input_mask: tf.int32, masked_lm_ids: tf.int32, masked_lm_positions: tf.int32, masked_lm_weights: tf.float32, next_sentence_labels: tf.int32, segment_ids: tf.int32}>\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0410 20:52:21.312966 140400511838080 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:Running eval on CPU\n",
            "I0410 20:52:21.313203 140400511838080 tpu_estimator.py:3124] Running eval on CPU\n",
            "INFO:tensorflow:*** Features ***\n",
            "I0410 20:52:21.313698 140400511838080 run_pretraining.py:141] *** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (32, 50)\n",
            "I0410 20:52:21.313894 140400511838080 run_pretraining.py:143]   name = input_ids, shape = (32, 50)\n",
            "INFO:tensorflow:  name = input_mask, shape = (32, 50)\n",
            "I0410 20:52:21.314074 140400511838080 run_pretraining.py:143]   name = input_mask, shape = (32, 50)\n",
            "INFO:tensorflow:  name = masked_lm_ids, shape = (32, 8)\n",
            "I0410 20:52:21.314244 140400511838080 run_pretraining.py:143]   name = masked_lm_ids, shape = (32, 8)\n",
            "INFO:tensorflow:  name = masked_lm_positions, shape = (32, 8)\n",
            "I0410 20:52:21.314431 140400511838080 run_pretraining.py:143]   name = masked_lm_positions, shape = (32, 8)\n",
            "INFO:tensorflow:  name = masked_lm_weights, shape = (32, 8)\n",
            "I0410 20:52:21.314633 140400511838080 run_pretraining.py:143]   name = masked_lm_weights, shape = (32, 8)\n",
            "INFO:tensorflow:  name = next_sentence_labels, shape = (32, 1)\n",
            "I0410 20:52:21.314796 140400511838080 run_pretraining.py:143]   name = next_sentence_labels, shape = (32, 1)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (32, 50)\n",
            "I0410 20:52:21.314957 140400511838080 run_pretraining.py:143]   name = segment_ids, shape = (32, 50)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/albert/modeling.py:194: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0410 20:52:21.315294 140400511838080 module_wrapper.py:139] From /usr/local/lib/python3.7/dist-packages/albert/modeling.py:194: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/albert/modeling.py:507: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0410 20:52:21.316882 140400511838080 module_wrapper.py:139] From /usr/local/lib/python3.7/dist-packages/albert/modeling.py:507: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/albert/modeling.py:588: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0410 20:52:21.339304 140400511838080 module_wrapper.py:139] From /usr/local/lib/python3.7/dist-packages/albert/modeling.py:588: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/albert/modeling.py:1025: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0410 20:52:21.391153 140400511838080 module_wrapper.py:139] From /usr/local/lib/python3.7/dist-packages/albert/modeling.py:1025: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/albert/modeling.py:253: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "W0410 20:52:22.663132 140400511838080 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/albert/modeling.py:253: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0410 20:52:22.664060 140400511838080 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "I0410 20:52:22.771111 140400511838080 run_pretraining.py:211] **** Trainable Variables ****\n",
            "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "I0410 20:52:22.771411 140400511838080 run_pretraining.py:217]   name = bert/embeddings/word_embeddings:0, shape = (30000, 128)\n",
            "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "I0410 20:52:22.771639 140400511838080 run_pretraining.py:217]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 128)\n",
            "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "I0410 20:52:22.771856 140400511838080 run_pretraining.py:217]   name = bert/embeddings/position_embeddings:0, shape = (512, 128)\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "I0410 20:52:22.772093 140400511838080 run_pretraining.py:217]   name = bert/embeddings/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "I0410 20:52:22.772270 140400511838080 run_pretraining.py:217]   name = bert/embeddings/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "I0410 20:52:22.772455 140400511838080 run_pretraining.py:217]   name = bert/encoder/embedding_hidden_mapping_in/kernel:0, shape = (128, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "I0410 20:52:22.772659 140400511838080 run_pretraining.py:217]   name = bert/encoder/embedding_hidden_mapping_in/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "I0410 20:52:22.772820 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "I0410 20:52:22.773001 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/query/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "I0410 20:52:22.773168 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "I0410 20:52:22.773354 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/key/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "I0410 20:52:22.773546 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "I0410 20:52:22.773754 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/self/value/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "I0410 20:52:22.773922 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "I0410 20:52:22.774103 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/attention_1/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "I0410 20:52:22.774317 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "I0410 20:52:22.774515 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0410 20:52:22.774710 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0410 20:52:22.774972 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0410 20:52:22.775178 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/kernel:0, shape = (3072, 768)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "I0410 20:52:22.775349 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/ffn_1/intermediate/output/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "I0410 20:52:22.775533 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/beta:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "I0410 20:52:22.775708 140400511838080 run_pretraining.py:217]   name = bert/encoder/transformer/group_0/inner_group_0/LayerNorm_1/gamma:0, shape = (768,)\n",
            "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0410 20:52:22.775907 140400511838080 run_pretraining.py:217]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0410 20:52:22.776128 140400511838080 run_pretraining.py:217]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "I0410 20:52:22.776315 140400511838080 run_pretraining.py:217]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 128)\n",
            "INFO:tensorflow:  name = cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "I0410 20:52:22.776525 140400511838080 run_pretraining.py:217]   name = cls/predictions/transform/dense/bias:0, shape = (128,)\n",
            "INFO:tensorflow:  name = cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "I0410 20:52:22.776704 140400511838080 run_pretraining.py:217]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (128,)\n",
            "INFO:tensorflow:  name = cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "I0410 20:52:22.776879 140400511838080 run_pretraining.py:217]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (128,)\n",
            "INFO:tensorflow:  name = cls/predictions/output_bias:0, shape = (30000,)\n",
            "I0410 20:52:22.777084 140400511838080 run_pretraining.py:217]   name = cls/predictions/output_bias:0, shape = (30000,)\n",
            "INFO:tensorflow:  name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0410 20:52:22.777264 140400511838080 run_pretraining.py:217]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "INFO:tensorflow:  name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "I0410 20:52:22.777490 140400511838080 run_pretraining.py:217]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0410 20:52:22.848954 140400511838080 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2022-04-10T20:52:22Z\n",
            "I0410 20:52:22.869062 140400511838080 evaluation.py:255] Starting evaluation at 2022-04-10T20:52:22Z\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0410 20:52:22.927643 140400511838080 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0410 20:52:23.020594 140400511838080 monitored_session.py:240] Graph was finalized.\n",
            "2022-04-10 20:52:23.021060: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-04-10 20:52:23.027410: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2022-04-10 20:52:23.027654: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e1bd1a5800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-04-10 20:52:23.027689: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-04-10 20:52:23.031795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-04-10 20:52:23.240024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-04-10 20:52:23.240936: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e1bd1a5d40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2022-04-10 20:52:23.240972: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2022-04-10 20:52:23.245645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-04-10 20:52:23.246378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-04-10 20:52:23.259784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-04-10 20:52:23.437732: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2022-04-10 20:52:23.521351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2022-04-10 20:52:23.540664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2022-04-10 20:52:23.715786: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2022-04-10 20:52:23.838359: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2022-04-10 20:52:24.203711: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-04-10 20:52:24.203928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-04-10 20:52:24.204874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-04-10 20:52:24.205596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2022-04-10 20:52:24.205708: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-04-10 20:52:24.207323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2022-04-10 20:52:24.207360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2022-04-10 20:52:24.207380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2022-04-10 20:52:24.207551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-04-10 20:52:24.208316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-04-10 20:52:24.209051: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2022-04-10 20:52:24.209105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10813 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-100\n",
            "I0410 20:52:24.212270 140400511838080 saver.py:1284] Restoring parameters from /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-100\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0410 20:52:27.944785 140400511838080 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0410 20:52:27.975495 140400511838080 session_manager.py:502] Done running local_init_op.\n",
            "2022-04-10 20:52:28.388464: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "I0410 20:52:31.726582 140400511838080 evaluation.py:167] Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "I0410 20:52:33.908851 140400511838080 evaluation.py:167] Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "I0410 20:52:36.067782 140400511838080 evaluation.py:167] Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "I0410 20:52:38.245676 140400511838080 evaluation.py:167] Evaluation [40/100]\n",
            "INFO:tensorflow:Evaluation [50/100]\n",
            "I0410 20:52:40.429584 140400511838080 evaluation.py:167] Evaluation [50/100]\n",
            "INFO:tensorflow:Evaluation [60/100]\n",
            "I0410 20:52:42.630331 140400511838080 evaluation.py:167] Evaluation [60/100]\n",
            "INFO:tensorflow:Evaluation [70/100]\n",
            "I0410 20:52:44.830591 140400511838080 evaluation.py:167] Evaluation [70/100]\n",
            "INFO:tensorflow:Evaluation [80/100]\n",
            "I0410 20:52:47.031056 140400511838080 evaluation.py:167] Evaluation [80/100]\n",
            "INFO:tensorflow:Evaluation [90/100]\n",
            "I0410 20:52:49.231699 140400511838080 evaluation.py:167] Evaluation [90/100]\n",
            "INFO:tensorflow:Evaluation [100/100]\n",
            "I0410 20:52:51.455461 140400511838080 evaluation.py:167] Evaluation [100/100]\n",
            "INFO:tensorflow:Finished evaluation at 2022-04-10-20:52:51\n",
            "I0410 20:52:51.507768 140400511838080 evaluation.py:275] Finished evaluation at 2022-04-10-20:52:51\n",
            "INFO:tensorflow:Saving dict for global step 100: global_step = 100, loss = 8.39913, masked_lm_accuracy = 0.14117458, masked_lm_loss = 8.392095, sentence_order_accuracy = 1.0, sentence_order_loss = 0.007035399\n",
            "I0410 20:52:51.508064 140400511838080 estimator.py:2049] Saving dict for global step 100: global_step = 100, loss = 8.39913, masked_lm_accuracy = 0.14117458, masked_lm_loss = 8.392095, sentence_order_accuracy = 1.0, sentence_order_loss = 0.007035399\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-100\n",
            "I0410 20:52:51.757550 140400511838080 estimator.py:2109] Saving 'checkpoint_path' summary for global step 100: /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-100\n",
            "INFO:tensorflow:evaluation_loop marked as finished\n",
            "I0410 20:52:51.758638 140400511838080 error_handling.py:101] evaluation_loop marked as finished\n",
            "INFO:tensorflow:***** Eval results *****\n",
            "I0410 20:52:51.758851 140400511838080 run_pretraining.py:557] ***** Eval results *****\n",
            "INFO:tensorflow:  global_step = 100\n",
            "I0410 20:52:51.764157 140400511838080 run_pretraining.py:560]   global_step = 100\n",
            "INFO:tensorflow:saving /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-100.meta to /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-best.meta\n",
            "I0410 20:52:52.206880 140400511838080 run_pretraining.py:568] saving /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-100.meta to /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-best.meta\n",
            "INFO:tensorflow:saving /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-100.data-00000-of-00001 to /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-best.data-00000-of-00001\n",
            "I0410 20:52:53.085421 140400511838080 run_pretraining.py:568] saving /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-100.data-00000-of-00001 to /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-best.data-00000-of-00001\n",
            "INFO:tensorflow:saving /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-100.index to /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-best.index\n",
            "I0410 20:52:55.159864 140400511838080 run_pretraining.py:568] saving /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-100.index to /content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-best.index\n",
            "INFO:tensorflow:  loss = 8.39913\n",
            "I0410 20:52:55.520941 140400511838080 run_pretraining.py:560]   loss = 8.39913\n",
            "INFO:tensorflow:  masked_lm_accuracy = 0.14117458\n",
            "I0410 20:52:55.521257 140400511838080 run_pretraining.py:560]   masked_lm_accuracy = 0.14117458\n",
            "INFO:tensorflow:  masked_lm_loss = 8.392095\n",
            "I0410 20:52:55.521457 140400511838080 run_pretraining.py:560]   masked_lm_loss = 8.392095\n",
            "INFO:tensorflow:  sentence_order_accuracy = 1.0\n",
            "I0410 20:52:55.521651 140400511838080 run_pretraining.py:560]   sentence_order_accuracy = 1.0\n",
            "INFO:tensorflow:  sentence_order_loss = 0.007035399\n",
            "I0410 20:52:55.521787 140400511838080 run_pretraining.py:560]   sentence_order_loss = 0.007035399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import torch\n",
        "from transformers import AlbertConfig, AlbertForPreTraining, load_tf_weights_in_albert\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, albert_config_file, pytorch_dump_path):\n",
        "    # Initialise PyTorch model\n",
        "    config = AlbertConfig.from_json_file(albert_config_file)\n",
        "    print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
        "    model = AlbertForPreTraining(config)\n",
        "\n",
        "    # Load weights from tf checkpoint\n",
        "    load_tf_weights_in_albert(model, config, tf_checkpoint_path)\n",
        "\n",
        "    # Save pytorch-model\n",
        "    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n",
        "    torch.save(model.state_dict(), pytorch_dump_path)\n",
        "\n",
        "\n",
        "\n",
        "convert_tf_checkpoint_to_pytorch('/content/gdrive/MyDrive/Albert_model_output_1/model.ckpt-100.index','/content/gdrive/MyDrive/Albert model/albert_base_3/assets/albert_config.json' , '/content/gdrive/MyDrive/Albert_model_output_1/pytorch_model.bin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0RJ7bhyJIdj",
        "outputId": "07364ada-0b3f-4850-8dad-e0029beeda6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building PyTorch model from configuration: AlbertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "Save PyTorch model to /content/gdrive/MyDrive/Albert_model_output_1/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N9WdiX4CPFGU",
        "outputId": "2dbbc8b3-e595-40a2-9846-a359b6c55f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 497.5 MB 26 kB/s \n",
            "\u001b[?25hCollecting tensorboard<2.9,>=2.8\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 38.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (0.24.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.44.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.14.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.21.5)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (13.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (3.10.0.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.8.0) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-2.8.0 tensorflow-2.8.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  AlbertConfig,TFAlbertModel\n",
        "from transformers import AlbertTokenizer\n",
        "from transformers import  AlbertConfig,TFAlbertModel\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")"
      ],
      "metadata": {
        "id": "7Ujp9hiXNX2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AlbertConfig.from_pretrained('/content/gdrive/MyDrive/Config_and_model', output_hidden_states=True)\n",
        "\n",
        "model = TFAlbertModel.from_pretrained('/content/gdrive/MyDrive/Config_and_model', config=config,  from_pt=True)\n",
        "print(model.config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeP5UMuhPIuU",
        "outputId": "50163f7e-226d-4f61-e96c-ef0b47071370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'albert.embeddings.position_ids', 'sop_classifier.classifier.weight', 'sop_classifier.classifier.bias']\n",
            "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlbertConfig {\n",
            "  \"_name_or_path\": \"/content/gdrive/MyDrive/Config_and_model\",\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# now load the training, validation and testing from the drive\n",
        "train_df = pd.read_csv(\"/content/gdrive/MyDrive/SCE_dataset/train.csv\")\n",
        "print(len(train_df['d_tokens'].values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yv7CtjrQXah",
        "outputId": "e538588d-1c05-4c51-a388-e6701185a90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# by using the Albert tokenization and trained albert model the docstring is embedded into 768 dimension vector\n",
        "avg_embeddings = []\n",
        "for count,item in enumerate(train_df['d_tokens'].values):\n",
        "    e = tokenizer.encode(item, max_length=512)\n",
        "    input = tf.constant(e)[None, :]\n",
        "    output = model(input)\n",
        "    v = [0]*768\n",
        "    for i in range(1, len(input[0])-1):\n",
        "        v = v + output[0][0][i].numpy()\n",
        "    avg_embeddings.append(v/len(input[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teEkXc8XR1Cz",
        "outputId": "3be0789c-fb23-41b5-b47b-b97e18934478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(avg_embeddings))\n",
        "# output confirms that all the docstring are converted into 768 dimension vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q__vJ_QvUoUP",
        "outputId": "17584959-1118-41c6-99ab-a26d534df27c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now save the embedding into tsv file for further use\n",
        "# Save the sentence embeddings in a .tsv file\n",
        "import csv\n",
        "with open(\"/content/gdrive/MyDrive/SCE_dataset/docstrin_embeddings.tsv\",\"w+\",newline='') as my_csv:\n",
        "    csvWriter = csv.writer(my_csv,delimiter='\\t')\n",
        "    csvWriter.writerows(avg_embeddings)"
      ],
      "metadata": {
        "id": "IkKbOs1vUvDw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}